# RunPod Serverless Integration for Wisteria CTR Studio

This directory contains setup scripts and documentation for using RunPod serverless endpoints for cost-effective distributed LLM inference.

## üöÄ **Why RunPod?**

- **60% Cost Savings**: $40-80 vs $600 for 1M predictions compared to OpenAI
- **Fast Cold Starts**: 30-90 seconds vs 5-15 minutes on GKE
- **Zero Idle Costs**: True serverless scaling to zero
- **Multiple GPU Options**: RTX 4090, A100, H100 support
- **Simple Setup**: No Kubernetes complexity

## üìä **Pricing Comparison**

| Model | GPU | Cost/Hour | Monthly (100h) | Use Case |
|-------|-----|-----------|----------------|----------|
| **Llama 8B** | RTX 4090 | $0.39 | $39 | Large populations, cost-effective |
| **Llama 70B** | A100 | $1.89 | $189 | High accuracy, smaller populations |

Compare to:
- OpenAI GPT-4o-mini: $600/1M predictions
- DeepSeek: $140/1M predictions

## üõ†Ô∏è **Setup Process**

### **1. Get RunPod API Key**
1. Go to [RunPod Console](https://runpod.io/console/user/settings)
2. Generate API key
3. Set environment variable:
   ```bash
   # Linux/Mac
   export RUNPOD_API_KEY="your-api-key-here"
   
   # Windows PowerShell
   $env:RUNPOD_API_KEY="your-api-key-here"
   ```

### **2. Automated Setup (Recommended)**
Run the setup script to automatically create endpoints:

```bash
# Linux/Mac
cd deploy/runpod
chmod +x setup-runpod.sh
./setup-runpod.sh

# Windows PowerShell
cd deploy/runpod
.\setup-runpod.ps1
```

The script will:
- Create optimized vLLM endpoints for Llama 8B and 70B
- Configure auto-scaling settings
- Generate environment configuration files

### **3. Load Configuration**
```bash
# Linux/Mac
source deploy/runpod/runpod-config.env

# Windows PowerShell
. deploy/runpod/runpod-config.ps1
```

### **4. Test the Integration**
```bash
# Test basic functionality
python deploy/runpod/test-runpod.py

# Run demo with RunPod
python demo.py --provider runpod --runpod-model llama-8b --ad "Test ad" --population-size 100
```

## üéÆ **Usage Examples**

### **Basic Usage**
```bash
# Cost-effective prediction with Llama 8B
python demo.py --provider runpod --runpod-model llama-8b --ad "Special offer!" --population-size 10000

# High accuracy with Llama 70B
python demo.py --provider runpod --runpod-model llama-70b --ad "Investment advice" --population-size 1000
```

### **Auto Model Selection**
```bash
# Automatically choose model based on population size
python demo.py --provider runpod --auto-model --ad "Tech gadgets" --population-size 25000
```

### **Always-On Pod (if available)**
```bash
# For consistent high-volume usage
python demo.py --provider runpod --runpod-pod-type pod --runpod-url "https://your-pod.runpod.net" --ad "Marketing campaign"
```

## ‚ö° **Performance Expectations**

### **Cold Start Times**
- **Serverless**: 30-90 seconds (much faster than GKE)
- **Always-on Pod**: Instant (0 seconds)

### **Throughput**
- **Llama 8B**: 1,000-3,000 predictions/second
- **Llama 70B**: 500-1,500 predictions/second

### **Cost per 1M Predictions**
- **Llama 8B**: $40-80 (depends on processing time)
- **Llama 70B**: $190-380

## üìù **Configuration Options**

### **Environment Variables**
```bash
export RUNPOD_API_KEY="your-api-key"                    # Required
export RUNPOD_LLAMA_8B_ENDPOINT="endpoint-id-for-8b"    # Auto-set by setup
export RUNPOD_LLAMA_70B_ENDPOINT="endpoint-id-for-70b"  # Auto-set by setup
```

### **Command Line Options**
```bash
--provider runpod                    # Use RunPod
--runpod-model {llama-8b,llama-70b}  # Model selection
--runpod-pod-type {serverless,pod}   # Deployment type
--runpod-endpoint ENDPOINT_ID        # Serverless endpoint ID
--runpod-url POD_URL                 # Always-on pod URL
--auto-model                         # Auto-select based on population size
```

## üîß **Troubleshooting**

### **Common Issues**

1. **"RunPod API key not found"**
   ```bash
   export RUNPOD_API_KEY="your-key-here"
   ```

2. **"Endpoint not configured"**
   ```bash
   # Run setup script to create endpoints
   ./setup-runpod.sh
   source runpod-config.env
   ```

3. **Cold start timeout**
   ```bash
   # Increase timeout or use always-on pod
   python demo.py --provider runpod --runpod-pod-type pod
   ```

## üéØ **Migration from Previous Setup**

The RunPod integration is a **drop-in replacement** for the previous distributed inference setup:

### **Before (Previous Setup)**
```bash
python demo.py --provider vllm --vllm-model llama-8b --vllm-url http://cluster-ip
```

### **After (RunPod)**
```bash
python demo.py --provider runpod --runpod-model llama-8b
```

### **Benefits of Migration**
- ‚úÖ **No Kubernetes complexity**
- ‚úÖ **60% cost reduction**
- ‚úÖ **Faster cold starts**
- ‚úÖ **True zero-cost idle**
- ‚úÖ **Professional GPU infrastructure**

## üìà **Scaling Strategy**

### **Development/Testing**
- Use **serverless** with **llama-8b**
- Auto-scale to zero when not used
- Cost: ~$0.39/hour only when active

### **Production (Low Volume)**
- Use **serverless** with model auto-selection
- Scale based on population size
- Cost: Variable based on usage

### **Production (High Volume)**
- Use **always-on pods** for consistent workloads
- Reserve capacity for guaranteed availability
- Cost: Fixed monthly rate with instant response

## üöÄ **Next Steps**

1. **Run setup**: `./setup-runpod.sh`
2. **Test integration**: `python demo.py --provider runpod --runpod-model llama-8b --ad "Test" --population-size 100`
3. **Scale up**: Use auto-model selection for production workloads
4. **Optimize**: Monitor costs and adjust models based on accuracy needs

RunPod provides a **production-ready, cost-effective** solution for distributed LLM inference without the complexity of managing Kubernetes clusters! üéâ